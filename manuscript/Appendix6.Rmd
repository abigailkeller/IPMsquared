---
title: "Appendix 6: Robustness Assessments"
output: pdf_document
bibliography: appendix1.bib
---
# Appendix 6

To assess model performance and robustness, we conducted both a model selection and model checking procedure. In Appendix 6.1, we describe the model selection procedure used to evaluate multiple functional forms for describing the overwinter mortality process using Watanabeâ€“Akaike information criterion (WAIC). In Appendix 6.2, we describe the model checking procedure and calculation of posterior predictive p-values. In Appendix 6.3, we evaluate the model fitted with a different mark-recapture dataset (D2) from Seadrift Lagoon in California, USA to demonstrate how posterior predictive checks can be useful in assessing the feasibility of integrating disparate datasets in the integrated population model.

## Appendix 6.1: Model selection

The inter-annual population transitions (i.e., transition from year $y$ to year $y+1$) are largely described by density-dependent overwinter mortality. Since density dependence only enters the model during this process and is therefore likely influential for forecasting the stable size distribution, we compared multiple functional forms for size- and density-dependent overwinter mortality (Equation 9 in main text).

We fit four separate models that are identical, apart from the functional form of overwinter survival (Equation 9). These models varied by whether the relationship between size- and density-dependence is additive or interactive, as well as how rapidly mortality decreases with size.

**Model 1:** interactive density- and size-dependence, steeper mortality decrease with size

$$
S_o(x,N^T_{t_\text{max},y}) = \text{exp}(-(\frac{\alpha_y^o \times N^T_{t_\text{max},y}}{x^2}) + \epsilon_y)
$$

**Model 2:** interactive density- and size-dependence, less steep mortality decrease with size

$$
S_o(x,N^T_{t_\text{max},y}) = \text{exp}(-(\frac{\alpha_y^o \times N^T_{t_\text{max},y}}{x}) + \epsilon_y)
$$

**Model 3:** additive density- and size-dependence, steeper mortality decrease with size

$$
S_o(x,N^T_{t_\text{max},y}) = \text{exp}(-(\frac{\alpha^o}{N^T_{t_\text{max},y}} + \frac{\psi}{x^2} + \epsilon_y))
$$

**Model 4:** additive density- and size-dependence, less steep mortality decrease with size

$$
S_o(x,N^T_{t_\text{max},i}) = \text{exp}(-(\frac{\alpha^o}{N^T_{t_\text{max},y}} + \frac{\psi}{x} + \epsilon_y))
$$

WAIC was calculated using every 10th sample from the combined posterior samples from four chains (3200 total samples). Model 1, with a size- and density-dependent overwinter mortality interaction and a steeper mortality decrease with size had the lowest WAIC and was used in subsequent analyses.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(kableExtra)
library(knitr)
df <- data.frame(
  Model = c("Model 1", "Model 2", "Model 3", "Model 4"),
  WAIC = c(6770.58, 6776.48, 6798.78, 7110.42),
  lppd = c(-3337.77, -3339.54, -3347.65, -3469.85),
  pWAIC = c(47.52, 48.70, 51.74, 85.36)
) %>% 
  mutate(delta_AIC = WAIC - min(WAIC))
colnames(df) <- c("Model", "WAIC", "lppd", "pWAIC", "$\\Delta$WAIC")

```

```{r, echo = FALSE}
# Render table
knitr::kable(
  df
)
```

## Appendix 6.2

Posterior predictive checks (PPCs) were used to determine if the model is an adequate representation of the observed data. The PPCs are based on the intuition that data simulated under a fitted model should be similar to the observed data the model was fitted to [@conn2018guide]. Calculating PPCs involves 1) cyclically drawing parameter values from the posterior distribution ($\mathbf{\theta}_i \sim [\mathbf{\theta}|\mathbf{D}]$), 2) generating a replicate data set for each $i$, $\mathbf{D}_i^\text{rep} \sim [\mathbf{D}|\mathbf{\theta}_i]$, calculating a discrepancy test statistic, $T(\mathbf{D}, \mathbf{\theta})$, for $\mathbf{D}$ and $\mathbf{D}^\text{rep}$, and comparing the distributions of test statistics of the observed data, $T(\mathbf{D}_i, \mathbf{\theta}_i)$, and replicated data, $T(\mathbf{D}_i^\text{rep}, \mathbf{\theta}_i)$. Bayesian p-values can be calculated based on these two distributions of test-statistics. 

We used two discrepancy functions, $T(\mathbf{D}, \mathbf{\theta})$, to test goodness of fit. Note that $\mathbf{\theta})$ includes both top-level parameters in the model (i.e., $x_{\infty}$, $\alpha$, etc.), as well as latent states/random effects in the model (i.e., $\sigma_y$, $\lambda^R_y$, etc.).

The first discrepancy function is an omnibus discrepancy function, deviance, which tests global lack of fit and was used by King et al. 2009 [@king2009bayesian]. Here, $D$ refers to the the count of removed crabs, $C_{t,j,y}(x_i)$ in the multi-year time-series data set (D1), the total number of removed crabs, $C^T_{t,i}(x_i)$ in the multi-year time-series data set (D1), the number of recaptured marked crabs, $r_t^{\text{mc}}(x_i)$ in the mark-recapture data set (D3), and the size of crabs in the size-at-age dataset (D2):

$$
T(D, \theta) = -2\text{log}(D|\theta)
$$

The second discrepancy function is a targeted discrepancy function that calculates the proportion of zeros and checks zero inflation in the count data [@conn2018guide]. We include this targeted measure since the count data in the multi-year time series data set (D1) is overdispersed due to spatial aggregation behavior of the crabs. Additionally, while posterior predictive $P$ values are known to be conservative [@conn2018guide], previous work suggests that discrepancy functions that are solely a function of simulated and observed data may be less conservative than those that depend on model parameters, $\theta$, like most omnibus discrepancy functions [@lunn2013bugs].

Here, $D$ refers to the the count of removed crabs, $C(x)_{t,j,y}$:

$$
T(D) = \sum_iI(D_i = 0)
$$

**Generating posterior predictive samples and calculating deviance**

The posterior predictive samples (PPS) and deviance of PPS was calculated using the following nimble function:

```{r, eval = FALSE}
library(nimble)

ppSamplerNF <- nimbleFunction(
  setup = function(model, samples) {
    
    # theta
    theta <- colnames(samples)
    theta <- model$topologicallySortNodes(theta)
    
    # nodes to simulate
    simNodes <- model$getNodeNames()[!(model$getNodeNames() %in% theta)]
    
    # data nodes
    dataNodes <- model$getNodeNames(dataOnly = TRUE)
    
    n <- length(model$expandNodeNames(dataNodes, returnScalarComponents = TRUE))
    vars <- colnames(samples[, theta])
    
    # subset posterior samples to just theta (top nodes in graph)
    samples_sub <- samples[, theta]
    
  },
  run = function(samples = double(2)) {
    
    nSamp <- dim(samples)[1]
    ppSamples <- matrix(nrow = nSamp, ncol = n + 1)
    
    for(i in 1:nSamp) {
      
      # add theta to model
      values(model, vars) <<- samples_sub[i, ]
      
      # update nodes based on theta
      model$simulate(simNodes, includeData = TRUE)
      
      # save PPS
      ppSamples[i, 1:n] <- values(model, dataNodes)
      
      # store deviance of each sample
      ppSamples[i, n + 1] <- 2 * model$calculate(dataNodes)
    }
    returnType(double(2))
    return(ppSamples)
  })
```

The posterior predictive samples (PPS) and deviance were calculated and saved using the nimble function:

```{r, eval = FALSE}
# generate posterior predictive sampler
ppSampler <- ppSamplerNF(
  myModel, # uncompiled model
  samples # posterior samples
)

# compile posterior predictive samples
cppSampler <- compileNimble(
  ppSampler, 
  project = CmyModel # compiled model
)

# run compiled function
ppSamples_via_nf <- cppSampler$run(samples)
```

**Calculating deviance of data**

Bayesian p-values are calculated by comparing the deviance of the posterior predictive samples and the deviance of the observed data. Deviance of the data was calculated using the following nimble function:

```{r, eval = FALSE}
calc_deviance_D <- nimbleFunction(
  setup = function(model, samples) {
    
    # theta = top nodes and latent states
    theta <- colnames(samples)
    
    # calculate model graph dependencies of theta to update
    deps <- model$getDependencies(theta, self = TRUE)
    
    # data nodes
    dataNodes <- model$getNodeNames(dataOnly = TRUE)
    
  },
  run = function(samples = double(2)) {
    
    nSamp <- dim(samples)[1]
    deviance <- rep(NA, nSamp)
    
    for(i in 1:nSamp) {
      
      # add theta
      values(model, theta) <<- samples[i, ]
      
      # update dependencies
      model$calculate(deps)
      
      # calculate deviance
      deviance[i] <- 2 * model$calculate(dataNodes)
    }
    returnType(double(1))
    return(deviance)
  })
```

The deviance of the data was calculated and saved using the nimble function:

```{r, eval = FALSE}
# generate deviance calculator
devianceCalculator <- calc_deviance_y(
  myModel, # uncompiled model, contains data
  samples # posterior samples
)

# compile deviance calculator
CdevianceCalculator <- compileNimble(
  devianceCalculator, 
  project = CmyModel # compiled model, contains data
)

# run compiled function
data_deviance_via_nf <- CdevianceCalculator$run(samples)
```

\newpage

**Model checking with deviance**

The Bayesian p-value calculated using deviance as the proportion of $T(D_i) > T(D^{\text{rep}}_i)$. The deviance p-value is 0.43. 

**Figure A6.1:** Histogram of deviance generated for each posterior sample for $D$ (red) and $D^{\text{rep}}$ (blue).

```{r, echo=FALSE}
knitr::include_graphics(
  paste0("C:/Users/abiga/Documents/Berkeley/Structured_Decision_Making/",
         "pop_dynamics_model/IPMsquared/figures/ppp_deviance_plot.png")
)
```

\newpage

**Model checked with zero inflation**

The Bayesian p-value using the targeted zero-inflation check was calculated as the proportion of $T(D_i) > T(D^{\text{rep}}_i)$, where $D$ refers to the count data, $C(x)_{t,j,y}$. The zero-inflation check p-value is 0.95. 

**Figure A6.2:** Histogram of proportion of zeros in each set, $i$, of posterior predictive samples, $D^{rep}$. Red line indicates the proportion of zeros in the count data, $D$.

```{r, echo=FALSE}
knitr::include_graphics(
  paste0("C:/Users/abiga/Documents/Berkeley/Structured_Decision_Making/",
         "pop_dynamics_model/IPMsquared/figures/ppp_zeros.png")
)
```

## Appendix 6.3: Testing model assumptions with posterior predictive checks

Integrated population models assume that the same population process underlies all data sets in the combined model, and estimation of "additional", previously unidentifiable parameters is particularly sensitive to violations of IPM model assumptions. Knowing whether the same population process has generated multiple data sets can be challenging, but posterior predictive checks may be a reliable way of testing model assumptions.

The mark-recapture data set (D2) was critical for inferring the trap hazard rates. The model described in the main text was fit using data from Roche Cove on Vancouver Island in British Columbia, Canada. Roche Cove is similar to Drayton Harbor, where the multi-year time series data (D1) was collected. Both sites have shallow, open tide flats and creeks that drain in the bay with similar estuary mouth habitats.

Another available mark-recapture data set is from Seadrift Lagoon in California, USA, taken from Grosholz et al. 2021, where the co-authors conducted an extensive capture-mark-recapture experiment to estimate green crab population size [@grosholz2021stage]. While the same Fukui traps were employed at Seadrift Lagoon as in Drayton Harbor and Roche Cove, the lagoon is small and man-made. The lagoon is so contained that both intra- and interspecies dynamics are likely different than expected in a more natural embayment. To test model assumptions and the sensitivity of our results to the choice of dataset, we fit the same model described in the main text, while replacing the Roche Cove mark-recapture data (D2) with the Seadrift Lagoon mark-recapture data. More details on the data and updated model can be found below in Appendix 6.3.1.

```{r, echo = FALSE, results = "hide"}
# read in samples
out_RC <- readRDS("../data/posterior_samples/savedsamples_IPM.rds")
out_SL <- readRDS("../data/posterior_samples/savedsamples_IPM_seadrift.rds")
```

```{r, echo = FALSE, results = "hide"}
# create df with param names
short_names <- c("h_F_max", "h_M_max", "h_S_max", 
                 "lambda_A", "lambda_R[1]", "lambda_R[2]", "lambda_R[3]",
                 "lambda_R[4]")

real_names <- c("h[F]^max", "h[M]^max", "h[S]^max",
                "lambda^A", "lambda[1]^R", "lambda[2]^R", "lambda[3]^R",
                "lambda[4]^R")

latex_names <- c("h_F^{max}", "h_M^{max}", "h_S^{max}",
                 "\\lambda^A", "\\lambda_1^R", "\\lambda_2^R",
                 "\\lambda_3^R", "\\lambda_4^R")

names_df <- as.data.frame(cbind(short_names, real_names, latex_names))
colnames(names_df) <- c("short_names", "real_names", "latex_names")
```

```{r, echo = FALSE}
library(bayestestR)
library(kableExtra)

# create posterior summary dataframe
out_RC_df <- rbind(out_RC[[1]], out_RC[[2]], 
                   out_RC[[3]], out_RC[[4]])[, short_names]
out_SL_df <- rbind(out_SL[[1]], out_SL[[2]], 
                   out_SL[[3]], out_SL[[4]])[, short_names]

posterior_summary <- as.data.frame(matrix(NA, nrow = length(short_names),
                                          ncol = 7))
colnames(posterior_summary) <- c("short_names", "mean_RC", "lower_ci_RC",
                                 "upper_ci_RC", "mean_SL", "lower_ci_SL",
                                 "upper_ci_SL")

for (i in seq_len(nrow(posterior_summary))) {
  posterior_summary[i, "short_names"] <- short_names[i]
  posterior_summary[i, "mean_RC"] <- mean(out_RC_df[, short_names[i]])
  posterior_summary[i, "lower_ci_RC"] <- as.numeric(
    hdi(out_RC_df[, short_names[i]], 0.95)[2])
  posterior_summary[i, "upper_ci_RC"] <- as.numeric(
    hdi(out_RC_df[, short_names[i]],0.95)[3])
  posterior_summary[i, "mean_SL"] <- mean(out_SL_df[, short_names[i]])
  posterior_summary[i, "lower_ci_SL"] <- as.numeric(
    hdi(out_SL_df[, short_names[i]], 0.95)[2])
  posterior_summary[i, "upper_ci_SL"] <- as.numeric(
    hdi(out_SL_df[, short_names[i]], 0.95)[3])
}

# round
for (i in seq_len(nrow(posterior_summary))) {
  
  dig <- 3 - floor(log10(abs(posterior_summary[i, "mean_RC"])))
  
  posterior_summary[i, "mean_RC"] <- round(posterior_summary[i, "mean_RC"], 
                                           digits = dig)
  posterior_summary[i, "lower_ci_RC"] <- round(posterior_summary[i, 
                                                                 "lower_ci_RC"], 
                                               digits = dig)
  posterior_summary[i, "upper_ci_RC"] <- round(posterior_summary[i, 
                                                                 "upper_ci_RC"],
                                               digits = dig)
  posterior_summary[i, "mean_SL"] <- round(posterior_summary[i, "mean_SL"], 
                                           digits = dig)
  posterior_summary[i, "lower_ci_SL"] <- round(posterior_summary[i, 
                                                                 "lower_ci_SL"], 
                                               digits = dig)
  posterior_summary[i, "upper_ci_SL"] <- round(posterior_summary[i, 
                                                                 "upper_ci_SL"], 
                                               digits = dig)
}

# combine
posterior_summary <- posterior_summary %>% 
  mutate(RC = paste0(mean_RC, " (", lower_ci_RC, ", ", upper_ci_RC, ")"),
         SL = paste0(mean_SL, " (", lower_ci_SL, ", ", upper_ci_SL, ")"))

posterior_summary <- left_join(posterior_summary, names_df, by = "short_names")

# get final df
posterior_final <- posterior_summary[, c("latex_names", "RC", "SL")]
colnames(posterior_final) <- c("parameter", "Roche Cove", "Seadrift Lagoon")
```

We first compare the parameter estimates of the maximum trap hazard rates, $H_F^\text{max}$, $H_M^\text{max}$, $H_S^\text{max}$, for Fukui, Minnow, and Shrimp traps, respectively, as well as the parameter estimates of green crab adult abundance in year 1, $\lambda^A$, and the recruit abundance for year $i$, $\lambda_i^R$. The estimates of the Fukui and Shrimp hazard rates are higher when the model is fitted with Seadrift Lagoon data, and the estimate of adult abundance in year 1 is lower when the model is fitted with Seadrift Lagoon data.

**Table A6:** Posterior summaries, including the mean and 95% credibility interval (highest density interval) with the model fit with the Roche Cove mark-recapture data (main text) and model fit with the Seadrift Lagoon mark-recapture data.

```{r, echo = FALSE}
# Process the table for parsed text
posterior_final <- posterior_final %>%
  mutate(parameter = paste0("$", parameter, "$"))

# Render table
knitr::kable(
  posterior_final,
  escape = FALSE,
  format = "latex",
  booktabs = TRUE,
  longtable = TRUE
) %>%
  kable_styling(full_width = FALSE, latex_options = c("repeat_header")) %>%
  kableExtra::row_spec(-1,
                       extra_latex_after = "\\renewcommand{\\arraystretch}{2}")
```

We then generated posterior predictive samples using the Seadrift Lagoon posterior and calculated Bayesian $P$ values based on deviance and proportion of zeros as described above.

The Bayesian p-value calculated using deviance as the proportion of $T(D_i) > T(D^{\text{rep}}_i)$. The deviance p-value is 0.71. 

**Figure A6.3:** Histogram of deviance generated for each posterior sample for $D$ (red) and $D^{\text{rep}}$ (blue) with the Seadrift Lagoon data.

```{r, echo=FALSE}
knitr::include_graphics(
  paste0("C:/Users/abiga/Documents/Berkeley/Structured_Decision_Making/",
         "pop_dynamics_model/IPMsquared/figures/ppp_deviance_plot_SL.png")
)
```

\newpage

**Model checked with zero inflation**

The Bayesian p-value using the targeted zero-inflation check was calculated as the proportion of $T(D_i) > T(D^{\text{rep}}_i)$, where $D$ refers to the count data, $C(x)_{t,j,y}$. The zero-inflation check p-value is 0.99. 

**Figure A6.4:** Histogram of proportion of zeros in each set, $i$, of posterior predictive samples, $D^{rep}$, calculated with the Seadrift Lagoon data. Red line indicates the proportion of zeros in the count data, $D$.

```{r, echo=FALSE}
knitr::include_graphics(
  paste0("C:/Users/abiga/Documents/Berkeley/Structured_Decision_Making/",
         "pop_dynamics_model/IPMsquared/figures/ppp_zeros_SL.png")
)
```

A Bayesian p-value of 0.5 would suggest an unbiased model. The p-values calculated with both discrepancy criteria with the Seadrift Lagoon data is farther from 0.5 than the p-values calculated with the Roche Cove data. We therefore conclude that the data-generating process at Drayton Harbor (D1) is more similar to the data-generating process at Roche Cove than Seadrift Lagoon, suggesting that the inference based on inclusion of the Roche Cove mark-recapture data is relatively less biased.

## Appendix 6.3.1: Seadrift Lagoon mark-recapture data

The authors in Grosholze et al., 2021 deployed 15 Fukui traps in June within the lagoon. Over the succeeding three days, they marked all crabs caught by clipping two spines on the carapace, recording their size, and releasing them back to their original location. Within successive weeks as part of an eradication program, crabs were removed, and the number and carapace width of retrieved (marked and unmarked) crabs were recorded. 

Data was retrieved from NSFâ€™s Biological and Chemical Oceanography Data Management Office at https://www.bco-dmo.org/person/699768. While the mark-recapture experiment took place from 2011-2018, we only used data from one year, 2011, since the marking data was unambiguous.

The Seadritf Lagoon mark-recapture data consists of 1) the count of marked and released crabs of discrete size $x_i$ in the first time period, $n^{\text{mc}}(x_i)$, and 2) the count of recaptured and marked crabs of discrete size $x_i$ in the second time period, $m^{\text{mc}}(x_i)$. Here we describe how the mark-recapture observation process is formulated with Seadrift Lagoon data (i.e., Equations 23-26 in main text):

These mark-recapture data follow a binomial distribution:

$$
m^{\text{mc}}(x_i) \sim \text{Binomial}(n^{\text{mc}}_{t_2}(x_i), p^{\text{mc}}(x_i)) 
$$

where $p^{\text{mc}}(x_i)$ is the total probability of capture based on the total number of Fukui traps set, $O^{\text{mc}}$, over the time period $\Delta b^{\text{mc}}$:

$$
p^{\text{mc}}(x) = 1-\text{exp}\left(-\sum_{j=1}^{O^{\text{mc}}} H_{F,j}(x)\Delta b^{\text{mc}}_j\right)
$$

These data also informed components of the growth and natural mortality kernel (Table 1; Figure 2). The number of marked and released crabs, $n^{\text{mc}}(x_i)$, at $t_1^{\text{mc}}$ underwent seasonal growth and natural mortality to the next time period, $t_2^{\text{mc}}$.

$$
n^{\text{mc}}_{t_2}(x') = \int_{x \in \Omega} K^{mc}(x',x) n^{\text{mc}}_{t_1}(x)dx
$$


# References

