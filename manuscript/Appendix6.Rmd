---
title: "Appendix 6: Robustness Assessments"
output: pdf_document
---
# Appendix 6

To assess model performance and robustness, we conducted both a model selection and model checking procedure. For model selection, we evaluated multiple functional forms for describing the overwinter mortality using Watanabeâ€“Akaike information criterion (WAIC). To check the model, we calculated posterior predictive p-values.

## Appendix 6.1

The inter-annual population transitions (i.e., transition from year $i$ to year $i+1$) are largely described by density-dependent overwinter mortality. Since density dependence only enters the model during this process and is therefore likely influential for forecasting the stable size distribution, we compared multiple functional forms for size- and density-dependent overwinter mortality (Equations 11-12 in main text).

We fit four separate models that are identical, apart from the functional form of overwinter survival (Equations 11-12). These models varied by whether the relationship between size- and density-dependence is additive or interactive, as well as how rapidly mortality decreases with size.

**Model 1:** interactive density- and size-dependence, steeper mortality decrease with size

$$
S_o(y,N^T_{t_{max},i}) = exp(-\frac{\alpha_i^o \times N^T_{t_{max},i}}{y^2})
$$

$$
\alpha^o_i \sim Gamma(\alpha^o_{\alpha}, \alpha^o_{\theta})
$$

**Model 2:** interactive density- and size-dependence, less steep mortality decrease with size

$$
S_o(y,N^T_{t_{max},i}) = exp(-\frac{\alpha_i^o \times N^T_{t_{max},i}}{y})
$$

$$
\alpha^o_i \sim Gamma(\alpha^o_{\alpha}, \alpha^o_{\theta})
$$

**Model 3:** additive density- and size-dependence, steeper mortality decrease with size

$$
S_o(y,N^T_{t_{max},i}) = exp(-(\frac{\alpha^o}{N^T_{t_{max},i}} + \frac{\psi}{y^2}))
$$

**Model 4:** additive density- and size-dependence, less steep mortality decrease with size

$$
S_o(y,N^T_{t_{max},i}) = exp(-(\frac{\alpha^o}{N^T_{t_{max},i}} + \frac{\psi}{y}))
$$

WAIC was calculated using the combined posterior samples from four chains, after discarding 2000 burn-in samples (32000 total samples). Model 1, with a size- and density-dependent overwinter mortality interaction and a steeper mortality decrease with size had the lowest WAIC and was used in subsequent analyses.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(kableExtra)
library(knitr)
df <- data.frame(
  Model = c("Model 1", "Model 2", "Model 3", "Model 4"),
  WAIC = c(6441.55, 6448.93, 6481.68, 6506.52),
  lppd = c(-3169.32, -3171.91, -3187.81, -3198.85),
  pWAIC = c(51.46, 52.56, 53.03, 54.41)
) %>% 
  mutate(delta_AIC = WAIC - min(WAIC))
colnames(df) <- c("Model", "WAIC", "lppd", "pWAIC", "$\\Delta$WAIC")

```

```{r, echo = FALSE}
# Render table
knitr::kable(
  df
)
```


## Appendix 6.2

Bayesian model checking was used to determine if the model is an adequate representation of the observed data. Fit was checked with an omnibus discrepancy function, deviance. Here, $y$ refers to the the count of removed crabs, $n^C_{t,j,i,y}$, the total number of removed crabs, $n^R_{t,i,y}$, and the number of recaptured marked crabs, $mc_y^{mc}$:

$$
T(y, \theta) = -2log(y|\theta)
$$

as well as a targeted discrepancy function to check zero inflation in the count data. Here, $y$ refers to the the count of removed crabs, $n^C_{t,j,i,y}$:

$$
T(y) = \sum_iI(y_i = 0)
$$

**Generating posterior predictive samples and calculating deviance**

The posterior predictive samples (PPS) and deviance of PPS was calculated using the following nimble function:

```{r, eval = FALSE}
library(nimble)

ppSamplerNF <- nimbleFunction(
  setup = function(model, samples) {
    
    # theta
    topNodes <- model$getNodeNames(stochOnly = TRUE, topOnly = TRUE)
    
    # nodes to simulate
    simNodes <- model$getNodeNames()[!(model$getNodeNames() %in% topNodes)]
    
    # data nodes
    dataNodes <- model$getNodeNames(dataOnly = TRUE)
    
    n <- length(model$expandNodeNames(dataNodes, returnScalarComponents = TRUE))
    vars <- colnames(samples[, topNodes])
    
    # subset posterior samples to just theta (top nodes in graph)
    samples_sub <- samples[, topNodes]
    
  },
  run = function(samples = double(2)) {
    
    nSamp <- dim(samples)[1]
    ppSamples <- matrix(nrow = nSamp, ncol = n + 1)
    
    for(i in 1:nSamp) {
      
      # add theta to model
      values(model, vars) <<- samples_sub[i, ]
      
      # update nodes based on theta
      model$simulate(simNodes, includeData = TRUE)
      
      # save PPS
      ppSamples[i, 1:n] <- values(model, dataNodes)
      
      # store deviance of each sample
      ppSamples[i, n + 1] <- 2 * model$calculate(dataNodes)
    }
    returnType(double(2))
    return(ppSamples)
  })
```

The posterior predictive samples (PPS) and deviance were calculated and saved using the nimble function:

```{r, eval = FALSE}
# generate posterior predictive sampler
ppSampler <- ppSamplerNF(
  myModel, # uncompiled model (see Appendix 3)
  samples # posterior samples
)

# compile posterior predictive samples
cppSampler <- compileNimble(
  ppSampler, 
  project = CmyModel # compiled model (see Appendix 3)
)

# run compiled function
ppSamples_via_nf <- cppSampler$run(samples)

# get PPS and samples
dataNodes <- myModel$getNodeNames(dataOnly = TRUE)
n <- length(CmyModel$expandNodeNames(dataNodes, returnScalarComponents = TRUE))
node_names <- myModel$expandNodeNames(dataNodes, returnScalarComponents = TRUE)
ppSamples <- ppSamples_via_nf[, 1:n]
deviance <- ppSamples_via_nf[, n + 1]

# save PPS (break up into smaller dfs)
for (i in 1:14) {
  temp <- grep(paste0("n_C\\[", i, ", "), node_names)
  saveRDS(node_names[temp], 
          paste0("data/posterior_predictive_check/node_names_",i,".rds"))
  saveRDS(ppSamples_via_nf[, temp],
          paste0("data/posterior_predictive_check/PPS_",i,".rds"))
}

# save deviance
saveRDS(deviance, "data/posterior_predictive_check/deviance_yrep.rds")
```

**Calculating deviance of data**

Bayesian p-values are calculated by comparing the deviance of the posterior predictive samples and the deviance of the observed data. Deviance of the data was calculated using the following nimble function:

```{r, eval = FALSE}
calc_deviance_y <- nimbleFunction(
  setup = function(model, samples) {
    
    # theta = top nodes and latent states
    theta <- colnames(samples)
    
    # calculate model graph dependencies of theta to update
    deps <- model$getDependencies(theta, self = TRUE)
    
    # data nodes
    dataNodes <- model$getNodeNames(dataOnly = TRUE)
    
  },
  run = function(samples = double(2)) {
    
    nSamp <- dim(samples)[1]
    deviance <- rep(NA, nSamp)
    
    for(i in 1:nSamp) {
      
      # add theta
      values(model, theta) <<- samples[i, ]
      
      # update dependencies
      model$calculate(deps)
      
      # calculate deviance
      deviance[i] <- 2 * model$calculate(dataNodes)
    }
    returnType(double(1))
    return(deviance)
  })
```

The deviance of the data was calculated and saved using the nimble function:

```{r, eval = FALSE}
# generate deviance calculator
devianceCalculator <- calc_deviance_y(
  myModel, # uncompiled model, contains data (see Appendix 3)
  samples # posterior samples
)

# compile deviance calculator
CdevianceCalculator <- compileNimble(
  devianceCalculator, 
  project = CmyModel # compiled model, contains data (see Appendix 3)
)

# run compiled function
data_deviance_via_nf <- CdevianceCalculator$run(samples)

# save
saveRDS(data_deviance_via_nf, "data/posterior_predictive_check/deviance_y.rds")
```

\newpage

**Model checking with deviance**

The Bayesian p-value calculated using deviance as the proportion of $T(y_i) > T(y^{rep}_i)$. The deviance p-value is 0.92. 

**Figure A6.1:** Histogram of deviance generated for each posterior sample for $y$ (red) and $y^{rep}$ (blue).

```{r, echo=FALSE}
knitr::include_graphics(
  paste0("C:/Users/abiga/Documents/Berkeley/Structured_Decision_Making/",
         "pop_dynamics_model/IPMsquared/figures/ppp_deviance_plot.png")
)
```

\newpage

**Model checked with zero inflation**

The Bayesian p-value using the targeted zero-inflation check was calculated as the proportion of $T(y_i) > T(y^{rep}_i)$, where $y$ refers to the count data. The zero-inflation check p-value is 0.92. 

**Figure A6.2:** Histogram of proportion of zeros in each set, $i$, of posterior predictive samples, $y^{rep}$. Red line indicates the proportion of zeros in the count data, $y$.

```{r, echo=FALSE}
knitr::include_graphics(
  paste0("C:/Users/abiga/Documents/Berkeley/Structured_Decision_Making/",
         "pop_dynamics_model/IPMsquared/figures/ppp_zeros.png")
)
```


